"""
Exam Analyzer Module
====================
Analyzes an exam PDF to extract its structure before grading.
This enables dynamic, exam-agnostic grading.
"""

import json
from pathlib import Path
from typing import Dict, Optional
from google import genai

import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import GOOGLE_API_KEY, GEMINI_MODEL
from utils.logger import setup_logger

logger = setup_logger("exam_analyzer")

# Cache for analyzed exams (path -> structure)
_exam_cache: Dict[str, dict] = {}

ANALYSIS_PROMPT = """Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø°ÙƒÙŠ. Ù…Ù‡Ù…ØªÙƒ ØªØ­Ù„ÙŠÙ„ ÙˆØ±Ù‚Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙŠÙƒÙ„Ù‡Ø§ Ø¨Ø¯Ù‚Ø©.

## Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
Ø­Ù„Ù„ ÙˆØ±Ù‚Ø© Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON:

```json
{
  "total_questions": <Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©>,
  "total_points": <Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª>,
  "questions": [
    {
      "number": <Ø±Ù‚Ù… Ø§Ù„Ø³Ø¤Ø§Ù„>,
      "title": "<Ø¹Ù†ÙˆØ§Ù† Ø£Ùˆ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„>",
      "type": "<Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„>",
      "sub_count": <Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø£Ùˆ Ø§Ù„ÙÙ‚Ø±Ø§Øª>,
      "points": <Ø¯Ø±Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„>,
      "requirement": "<Ù…Ø§ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù† Ø§Ù„Ø·Ø§Ù„Ø¨>",
      "special_instructions": "<Ø£ÙŠ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ù…Ø«Ù„ 'Ø§Ø®ØªØ± ÙˆØ§Ø­Ø¯Ø§Ù‹' Ø£Ùˆ 'Ø£Ø¬Ø¨ Ø¹Ù† Ø¬Ù…ÙŠØ¹'>"
    }
  ]
}
```

## Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ù…ÙƒÙ†Ø©:
- "theoretical": Ø£Ø³Ø¦Ù„Ø© Ù†Ø¸Ø±ÙŠØ© ØªØªØ·Ù„Ø¨ Ø´Ø±Ø­ Ø£Ùˆ ØªØ¹Ø±ÙŠÙ
- "comparison": Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ù…ÙÙ‡ÙˆÙ…ÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø±
- "experiment": ØªØ¬Ø±Ø¨Ø© Ø£Ùˆ Ù†Ø´Ø§Ø· Ø¹Ù…Ù„ÙŠ
- "math": Ù…Ø³Ø§Ø¦Ù„ Ø­Ø³Ø§Ø¨ÙŠØ©
- "choose_one": Ø§Ø®ØªØ± ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù…Ù† Ø¹Ø¯Ø© Ø®ÙŠØ§Ø±Ø§Øª
- "mixed": Ù†ÙˆØ¹ Ù…Ø®ØªÙ„Ø·

## ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¨Ø­Ø« Ø¹Ù† Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "Ø§Ø®ØªØ±" Ø£Ùˆ "Ø£Ø¬Ø¨ Ø¹Ù† ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·" Ø£Ùˆ "Ø£Ø­Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠÙ†"
2. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª "Ø§Ø®ØªØ± ÙˆØ§Ø­Ø¯Ø§Ù‹" Ø£Ùˆ Ù…Ø§ ÙŠØ´Ø§Ø¨Ù‡Ù‡Ø§ØŒ Ø¶Ø¹ requirement: "choose_one"
3. Ø§Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø¨Ø¯Ù‚Ø© (1-ØŒ 2-ØŒ Ø£)ØŒ Ø¨)ØŒ Ø¥Ù„Ø®)
4. Ø¥Ø°Ø§ Ù„Ù… ØªØ­Ø¯Ø¯ Ø§Ù„Ø¯Ø±Ø¬Ø§ØªØŒ Ø§ÙØªØ±Ø¶ ØªÙˆØ²ÙŠØ¹Ø§Ù‹ Ù…ØªØ³Ø§ÙˆÙŠØ§Ù‹
5. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†ÙˆØ¹ "choose_one"ØŒ Ø¶Ø¹ sub_count ÙƒØ¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

## Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:
```json
{
  "total_questions": 4,
  "total_points": 100,
  "questions": [
    {
      "number": 1,
      "title": "Ø£Ø³Ø¦Ù„Ø© Ù†Ø¸Ø±ÙŠØ© Ø¹Ù† Ø§Ù„Ù…ØªØ³Ø¹Ø§Øª",
      "type": "theoretical",
      "sub_count": 10,
      "points": 25,
      "requirement": "answer_all",
      "special_instructions": "Ø£Ø¬Ø¨ Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª"
    },
    {
      "number": 2,
      "title": "Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¹ÙˆØ§Ø²Ù„",
      "type": "comparison",
      "sub_count": 1,
      "points": 25,
      "requirement": "complete",
      "special_instructions": null
    },
    {
      "number": 3,
      "title": "ØªØ¬Ø±Ø¨Ø© Ø£Ùˆ Ù†Ø´Ø§Ø·",
      "type": "choose_one",
      "sub_count": 2,
      "points": 25,
      "requirement": "choose_one",
      "special_instructions": "Ø§Ø®ØªØ± Ø£Ø­Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠÙ† ÙÙ‚Ø· Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"
    }
  ]
}
```

Ø£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.
"""


class ExamAnalyzer:
    """Analyzes exam PDFs to extract structure for grading"""
    
    def __init__(self):
        """Initialize the Gemini client"""
        self.client = genai.Client(api_key=GOOGLE_API_KEY)
        logger.info("ExamAnalyzer initialized")
    
    def analyze_exam(self, exam_path: Path, force_refresh: bool = False) -> dict:
        """
        Analyze an exam PDF and extract its structure.
        
        Args:
            exam_path: Path to the exam PDF
            force_refresh: If True, re-analyze even if cached
            
        Returns:
            Dictionary with exam structure
        """
        path_key = str(exam_path)
        
        # Check cache first
        if not force_refresh and path_key in _exam_cache:
            logger.info(f"Using cached analysis for: {exam_path.name}")
            return _exam_cache[path_key]
        
        logger.info(f"Analyzing exam: {exam_path.name}")
        
        try:
            # Upload the PDF
            exam_file = self.client.files.upload(file=exam_path)
            logger.info("Exam PDF uploaded to Gemini")
            
            # Send analysis request
            response = self.client.models.generate_content(
                model=GEMINI_MODEL,
                contents=[
                    exam_file,
                    ANALYSIS_PROMPT
                ]
            )
            
            # Parse JSON response
            response_text = response.text.strip()
            
            # Extract JSON from response (handle markdown code blocks)
            if "```json" in response_text:
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                response_text = response_text[start:end].strip()
            elif "```" in response_text:
                start = response_text.find("```") + 3
                end = response_text.find("```", start)
                response_text = response_text[start:end].strip()
            
            structure = json.loads(response_text)
            
            # Cache the result
            _exam_cache[path_key] = structure
            
            logger.info(f"Exam analysis complete: {structure.get('total_questions')} questions")
            self._log_structure(structure)
            
            return structure
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse exam structure JSON: {e}")
            logger.error(f"Raw response: {response_text[:500]}")
            return self._get_default_structure()
        except Exception as e:
            logger.error(f"Error analyzing exam: {e}")
            return self._get_default_structure()
    
    def _log_structure(self, structure: dict):
        """Log the extracted structure for debugging"""
        logger.info(f"Total questions: {structure.get('total_questions')}")
        logger.info(f"Total points: {structure.get('total_points')}")
        for q in structure.get("questions", []):
            req = q.get("requirement", "answer_all")
            special = q.get("special_instructions", "")
            logger.info(
                f"  Q{q['number']}: {q['title'][:30] if 'title' in q else 'No title'} | "
                f"{q['type']} | {q['sub_count']} subs | "
                f"{q['points']} pts | {req} | {special[:30] if special else 'none'}"
            )
    
    def _get_default_structure(self) -> dict:
        """Return a safe default structure if analysis fails"""
        return {
            "total_questions": 4,
            "total_points": 100,
            "questions": [
                {"number": i, "type": "mixed", "sub_count": 5, 
                 "points": 25, "requirement": "answer_all", "special_instructions": None}
                for i in range(1, 5)
            ]
        }
    
    def get_grading_context(self, structure: dict) -> str:
        """
        Generate grading context string from exam structure.
        This will be included in the grading prompt.
        """
        context_lines = [
            "## ğŸ¯ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† (ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹):",
            f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©**: {structure.get('total_questions', 4)}",
            f"- **Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª**: {structure.get('total_points', 100)}",
            ""
        ]
        
        for q in structure.get("questions", []):
            q_num = q.get("number", "?")
            q_type = q.get("type", "mixed")
            sub_count = q.get("sub_count", 1)
            points = q.get("points", 25)
            requirement = q.get("requirement", "answer_all")
            special = q.get("special_instructions", "")
            
            context_lines.append(f"### Ø§Ù„Ø³Ø¤Ø§Ù„ {q_num}:")
            context_lines.append(f"- **Ø§Ù„Ù†ÙˆØ¹**: {q_type}")
            context_lines.append(f"- **Ø¹Ø¯Ø¯ Ø§Ù„ÙÙ‚Ø±Ø§Øª/Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡**: {sub_count}")
            context_lines.append(f"- **Ø§Ù„Ø¯Ø±Ø¬Ø©**: {points}")
            
            # Special handling for choose_one - FIXED
            if requirement == "choose_one":
                context_lines.append(f"- **âš ï¸ Ù…Ù‡Ù…**: Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…Ù† Ù†ÙˆØ¹ 'Ø§Ø®ØªØ± ÙˆØ§Ø­Ø¯Ø§Ù‹'")
                context_lines.append(f"- **Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ®ØªØ§Ø± Ø®ÙŠØ§Ø±Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙÙ‚Ø· Ù…Ù† {sub_count} Ø®ÙŠØ§Ø±Ø§Øª!")
                context_lines.append(f"- **ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª**: Ø¥Ø°Ø§ Ø£Ø¬Ø§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù‰ Ø®ÙŠØ§Ø± ÙˆØ§Ø­Ø¯ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ ÙˆØµØ­ÙŠØ­ = {points} Ù†Ù‚Ø·Ø© ÙƒØ§Ù…Ù„Ø©!")
                context_lines.append(f"- **âš ï¸ ØªØ­Ø°ÙŠØ±**: Ù„Ø§ ØªÙ‚Ø³Ù… Ø§Ù„Ø¯Ø±Ø¬Ø© Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª. Ø¥Ø°Ø§ Ø£Ø¬Ø§Ø¨ Ø¹Ù„Ù‰ Ø®ÙŠØ§Ø± ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ = {points}/{points}")
                context_lines.append(f"- **Ø¥Ø°Ø§ Ø£Ø¬Ø§Ø¨ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† Ø®ÙŠØ§Ø±**: Ø®Ø° Ø£ÙˆÙ„ Ø®ÙŠØ§Ø± ÙÙ‚Ø· ÙˆØ§Ø¹Ø·ÙŠÙ‡ {points} Ù†Ù‚Ø·Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† ØµØ­ÙŠØ­Ø§Ù‹")
            elif requirement == "complete":
                context_lines.append(f"- **Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§Ù…Ù„Ø© Ù…ØªÙƒØ§Ù…Ù„Ø©")
            else:
                points_per_sub = points / sub_count if sub_count > 0 else points
                context_lines.append(f"- **Ø§Ù„Ù…Ø·Ù„ÙˆØ¨**: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª")
                context_lines.append(f"- **ÙƒÙ„ ÙÙ‚Ø±Ø© = {points_per_sub:.1f} Ù†Ù‚Ø·Ø©**")
            
            if special:
                context_lines.append(f"- **Ù…Ù„Ø§Ø­Ø¸Ø© Ø®Ø§ØµØ©**: {special}")
            
            context_lines.append("")
        
        return "\n".join(context_lines)


# Singleton instance
_analyzer: Optional[ExamAnalyzer] = None

def get_analyzer() -> ExamAnalyzer:
    """Get or create the exam analyzer singleton"""
    global _analyzer
    if _analyzer is None:
        _analyzer = ExamAnalyzer()
    return _analyzer


def analyze_exam(exam_path: Path, force_refresh: bool = False) -> dict:
    """Convenience function to analyze an exam"""
    return get_analyzer().analyze_exam(exam_path, force_refresh)


def get_grading_context(exam_path: Path) -> str:
    """Get grading context for an exam (analyzes if needed)"""
    structure = analyze_exam(exam_path)
    return get_analyzer().get_grading_context(structure)
